# originally created because wget -i crashed on my server because it ran out of memory
# downloads all href links on a page
# I used it with the page generated by https://www.flickandshare.com, but should work for almost any page. 

import urllib
import lxml.html

url = 'https://google.com'
connection = urllib.urlopen(url)

dom =  lxml.html.fromstring(connection.read())
count = 1
failed = []
for link in dom.xpath('//a/@href'): # select the url in href for all a tags(links)
	try:
		image = urllib.URLopener()
		image.retrieve(link, str(count) + ".jpg")
		count += 1
	except:
		failed.append(link)

print failed
